<!DOCTYPE html>
<!-- VeP http://web.unibas.it/bloisi/corsi/visione-e-percezione.html -->
<html lang="en"><!--<![endif]--><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Plastic in River Detector</title>
	<!-- Meta -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Detection of plastic in rivers">
    <meta name="author" content="Bonelli Antonella, Calocero Simona and Rosa Marco adapted a 3rd Wave Media template">
    <link rel="shortcut icon" href="http://web.unibas.it/bloisi/tutorial/favicon.ico">  
    <link href="plastic_in_river_files/css.txt" rel="stylesheet" type="text/css">
    <link href="plastic_in_river_files/css1.txt" rel="stylesheet" type="text/css">
    <!-- Global CSS -->
    <link rel="stylesheet" href="plastic_in_river_files/bootstrap.css">
    <!-- Plugins CSS -->
    <link rel="stylesheet" href="plastic_in_river_files/font-awesome.css">
        
    <!-- Theme CSS -->  
    <link id="theme-style" rel="stylesheet" href="plastic_in_river_files/styles.css">
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <script>
        MathJax = {
          tex: {
            inlineMath: [['$', '$'], ['\\(', '\\)']]
          }
        };
        </script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    <script src="plastic_in_river_files/splide.min.js"></script>
    <link rel="stylesheet" href="plastic_in_river_files/splide.min.css">
    <script src="plastic_in_river_files/splide-extension-video.min.js"></script>
    <link rel="stylesheet" href="plastic_in_river_files/splide-extension-video.min.css">
    <script>
            document.addEventListener('DOMContentLoaded', function () {
                Splide.defaults = {
                    keyboard: true,
                    paginationKeyboard: true,
                }

                

                let video = new Splide('#video-carousel', {
                    type     : 'loop',
                    height   : 'full',
                    focus    : 'center',
                    gap:    '5rem',
                    autoWidth: true,
                });
                video.mount(window.splide.Extensions);
                let confusion = new Splide('#confusion-carousel', {
                    type     : 'loop',
                    width    : '95%',
                    focus    : 'center',
                    autoHeight: true,
                });
                confusion.mount();
            });
    </script>
</head> 

<body>
    <!-- ******HEADER****** --> 
    <header class="header">
        <div class="container">
            <img class="profile-image img-responsive pull-left" src="assets/bottle_in_river_annotated.jpg"
                 alt="Plastic bottle in river" width="300"/>
            <div class="profile-content pull-right">
				<img class="profile-image img-responsive pull-left"
				    src="http://web.unibas.it/bloisi/assets/images/logo.png"
					alt="unibas logo" height=97 width=312/>
				<p>&nbsp;</p>
				<h3 class="desc">
                    <a href="http://web.unibas.it/bloisi/corsi/visione-e-percezione.html" target="_blank">
                        Corso di Visione e Percezione
                    </a>
				</h3>
			</div>
            <div class="profile-content pull-left">
                <h1 class="name">Plastic in River Detector</h1>
                <h2 class="desc">Computer Vision for the environment</h2>
            </div>


        </div><!--//container-->
    </header><!--//header-->
    
    <div class="container sections-wrapper">
        <div class="row">
            <div class="primary col-md-8 col-sm-12 col-xs-12">
			
			    <section class="about section">
                    <div class="section-inner">
                        <h2 class="heading" id="introduction">Introduction</h2>
                        <div class="content">
                            <p>
                                This project aims to train a <b>CNN</b> (Convolutional Neural Network) model for the detection
                                and classification of <b>plastic pollution in rivers</b>.<br/>
                                The detector will classify objects into four distinct classes:
                                <ul>
                                    <li>plastic bottles</li>
                                    <li>plastic bags</li>
                                    <li>other plastic waste</li>
                                    <li>non-plastic items</li>
                               </ul>
                                For the realization we will use the <b>YOLO</b> (You Only Look Once) algorithm and the <b>Darknet</b> framework,
                                all working on a <b>Colab</b> notebook.
							</p>
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading" id="motivations">Motivations</h2>
                        <div class="content">
                            <p>
                                Detecting plastic in rivers is a significant environmental challenge due to the increasing
                                widespread plastic pollution of waterways and hence of oceans.</br>
                                Monitor rivers with cameras placed along them that are able to detect in real time
                                the presence of plastic in water could be very useful, for instance notifying the competent authorities
                                in order to clean up the place.
                            </p>
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
			
                <section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="goals"></a>Goals</h2>
                        <div class="content">
                            <p>
                                Project main objectives:
                            </p>
							<ol>
								<li>Explore the dataset</li>
								<li>Train a CNN</li>
								<li>Detect objects of interest</li>
							</ol>
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="method"></a>Method</h2>
                        <div class="content">			
                            <h4>Object Detection</h4>

                            <p>YOLO (You Only Look Once) is a real-time object detection algorithm first developed
                                in
                                2015. It is a single-stage object detector that uses a convolutional neural network
                                (CNN) to predict the bounding boxes and class probabilities of objects in input
                                images.
                            </p>

                            <p>One of the key advantages of YOLO is that it processes the entire image in one
                                forward
                                pass, making it faster and more efficient than two-stage object detector methods, as
                                it
                                eliminates the need for multiple stages making it suitable for applications such as
                                real-time video analytics even on low-powered devices.</p>

                            <p>The basic idea behind YOLO algorithm can be broken down into several steps:</p>

                            <ol>
                                <li>Input image is passed through a CNN to extract features from the image.</li>
                                <li>The features are then passed through a series of fully connected layers, which
                                    predict class probabilities and bounding box coordinates.</li>
                                <li>The image is divided into a grid of cells, and each cell is responsible for
                                    predicting a set of bounding boxes and class probabilities.</li>
                                <li>The output of the network is a set of bounding boxes and class probabilities for
                                    each cell.</li>
                                <li>The bounding boxes are then filtered using a post-processing algorithm called
                                    non-max suppression to remove overlapping boxes and choose the box with the
                                    highest
                                    probability.
                                </li>
                                <li>The final output is a set of predicted bounding boxes and class labels for each
                                    object in the image.</li>
                            </ol>
                            <div style="display: flex; justify-content: center; margin: 2rem;"><img src="assets/yolo.png" style="width: 80%;"></div>
                            <p>YOLO has been developed in several versions, including YOLOv1 through YOLOv8. Each
                                version has been built on top of the previous version
                                with enhanced features such as improved accuracy, faster processing, and better
                                handling
                                of small objects.</p>
                            <p>In this project In this project, we used the latest iteration of these YOLO models:
                                <b>YOLOv8</b> released in 2023 by <a
                                    href="https://docs.ultralytics.com/">Ultralytics</a>.
                            </p>
                            <p>YOLO models are pre-trained on huge datasets such as COCO and ImageNet.
                                They provide highly accurate predictions on classes they are pre-trained on
                                and can also give the opportunity to fine-tune the model on a custom dataset.
                            </p>
                            <p>Fine-tuning involves training the model on new data for a certain number of epochs.
                                During fine-tuning, the model adjusts its parameters to better adapt to the
                                characteristics and objects present in the dataset.</p>

                            <h4>Dataset Preparation</h4>
                            <p>The dataset used in this project was created using proprietary images and images
                                obtained
                                from RoboFlow Universe. Specifically, <a
                                    href="https://orca-tech.cn/en/datasets/FloW/FloW-Img">FloW-Img</a> and <a
                                    href="https://universe.roboflow.com/ipsa-4wlge/wcb5g/dataset/17">WCB5G</a> were
                                combined.</p>
                            <p>Annotations were standardized into two categories:
                            <ul>
                                <li><b>WASTE</b></li>
                                <li><b>DEBRIS</b></li>
                            </ul>
                            </p>
                            <p>The YOLO format was used for annotations. The <i>*.txt</i> file are formatted with
                                one line for each object in the image following the provided format:
                            <pre>class x_center y_center width height</pre>
                            Bounding box coordinates should be in the normalized xywh format (ranging from 0 to 1).
                            If box coordinates are expressed in pixels, x_center and width should be divided by the
                            image width, and y_center and height should be divided by the image height. Class
                            numbers should be zero-indexed (start with 0).</p>
                            <p>Regarding the test split of the dataset, we recorded videos and sampled sampled every
                                x seconds. Then we manually labeled the classes using <a
                                    href="https://app.roboflow.com/">RoboFlow</a>.</p>

                            <img src="assets/roboflow.jpeg" style="width: 100%;"></br>
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="implementation"></a>Implementation and Code</h2>
                        <div class="content">
                            <p>Descrivere la tecnologia utilizzata per risolvere il problema</p>
							<p>Per esempio:<br>
							<ul>
							    <li>Python 3</li>
								<li>ROS Melodic</li>
								<li>OpenCV 4.2</li>								
                            </ul>
							</p>
							
							<p style="color: red">Link al repository Git con il codice<br>
							Il codice deve contenere un file README con le istruzioni per eseguire il codice</p>
                                                     
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="dataset"></a>Dataset</h2>
                        <div class="content">
                            <p>The dataset consists of approximately 1000 images.</p>
                            <img src="assets/labels.jpg" style="width: 100%;">
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
				
				<section class="about section">
                    <div class="section-inner">
                        <h2 class="heading"><a id="results"></a>Results</h2>
                        <div class="content">
							<h3>Qualitative Results</h3>
                            <div id="video-carousel" class="splide">
                                <div class="splide__track">
                                  <ul class="splide__list">
                                    <li class="splide__slide" data-splide-youtube="https://www.youtube.com/watch?v=gbc3Odf51Ec">
                                     <img src="https://img.youtube.com/vi/gbc3Odf51Ec/hqdefault.jpg">
                                    </li>
                                    <li class="splide__slide" data-splide-youtube="https://www.youtube.com/watch?v=AEkw8cpiJr4">
                                        <img src="https://img.youtube.com/vi/AEkw8cpiJr4/hqdefault.jpg">
                                    </li>
                                    <li class="splide__slide" data-splide-youtube="https://www.youtube.com/watch?v=0a7Bvfw9LXU">
                                        <img src="https://img.youtube.com/vi/0a7Bvfw9LXU/hqdefault.jpg">
                                    </li>
                                    <li class="splide__slide" data-splide-youtube="https://www.youtube.com/watch?v=Ehtl8rn1yM8">
                                        <img src="https://img.youtube.com/vi/Ehtl8rn1yM8/hqdefault.jpg">
                                    </li>
                                    <li class="splide__slide" data-splide-youtube="https://www.youtube.com/watch?v=AUR7rTMQq4M">
                                        <img src="https://img.youtube.com/vi/AUR7rTMQq4M/hqdefault.jpg">
                                    </li>
                                  </ul>
                                </div>
                              </div>
							<h2>Quantitative Results</h3>
							The following section will list the metrics used to measure the performance of the detection model.
                            <h3>Definitions</h3>
                            <h4>Intersection Over Union (IOU)</h4>
                            <p>Intersection Over Union (IOU) is a measure that evaluates the overlap between two bounding boxes.
                                It requires a ground truth bounding box and a predicted bounding box.</br>
                                IOU is given by the overlapping area between the predicted bounding box and the ground truth bounding box divided by
                                the area of union between them:
                            <div style="display: flex; justify-content: center; margin: 2rem;"><img src="assets/iou.jpeg" style="width: 50%;"></div>
                            where "Area of Intersection" is the area where the two bounding boxes overlap, and "Area of Union" is the total area
                            covered by both bounding boxes. The IoU gives a value between 0 and 1, where 0 indicates no overlap and 1 indicates
                            complete overlap. We can determine whether the detection is correct (True Positive) or not (False Positive) by applying
                            a threshold.
                            </p>
                            <p>
                            <ul>
                                <li>✅ <b>True Positive (TP)</b>: A correct detection. Detection with IOU ≥ threshold.<br>
                                    The model <b>predicted</b> a label and the entity associated with the
                                    label <b>was present</b> in the image.</li>
                                <li>❌ <b>False Positive (FP)</b>: A wrong detection. Detection with IOU &lt; threshold.<br>
                                    The model <b>predicted</b> a label and entity associated with the label
                                    <b>was not present</b> in the image.
                                </li>
                                <li>❌ <b>False Negative (FN)</b>:
                                    The model <b>didn't predict</b> a label and the entity associated with the label <b>was present</b> in the
                                    image.</li>
                                <li>✅ <b>True Negative (TN)</b>:
                                    The model <b>didn't predict</b> a label and the entity associated with the label <b>was not present</b> in the
                                    image.<br>
                                    In the object detection task there are many possible bounding boxes that should not be detected within an image.
                                    Thus, TN would be all possible bounding boxes that were corrrectly not detected (so many possible boxes within
                                    an image). That's why it is not used by the metrics.</li>
                            </ul>
                            </p>
                            <h4>Precision</h4>
                            <p>Precision is the ability of a model to identify only the relevant objects. It is the percentage of correct positive
                                predictions and is given by:$$ P = {TP \over TP + FP} = {TP \over all\; detection}$$
                                The denominator represents the total number of bounding boxes proposed by the model.</p>
                            <h4>Recall</h4>
                            <p>Recall is the ability of a model to find all the relevant cases (all ground truth bounding boxes). It is the
                                percentage of true positive detected among all relevant ground truths and is given by:
                                $$ R = {TP \over TP + FN} = {TP \over all\; ground\; truth}$$
                                The denominator represents the number of ground truth bounding boxes.</p>
                            <h4>Mean Average Precision</h4>
                            <p>Another numerical metric for evaluating performance is the Average Precision (AP). It is obtained by calculating the
                            area under the Precision x Recall curve interpolated at 11 points for each of <i>n</i> the class. Once the average precision for each
                            class is obtained, the Mean Average Precision (mAP) is calculated as follows: 
                            $$ mAP = {1 \over n}{\sum_{i=1}^{n} AP_{i}} $$ 
                            </p>
                            <div style="display: flex; justify-content: center; margin: 2rem;"><img src="assets/results.png" style="width: 100%;"></div>
                            <h4>Confusion Matrix</h4>
                            <p>The confusion matrix is a method for summarizing a classification algorithm’s performance.
                                The diagonal line shows the significance of the prediction outcomes in the confusion matrix; the horizontal and vertical lines represent false negatives and false positives, respectively.
                                <section id="confusion-carousel" class="splide">
                                    <div class="splide__track">
                                        <ul class="splide__list">
                                            <li class="splide__slide center">
                                                <div>
                                                    <img src="assets/confusion_matrix_val.png" alt="">
                                                    <h4>Validation</h4>
                                                </div>
                                            </li>
                                            <li class="splide__slide">
                                                <div>
                                                    <img src="assets/confusion_matrix_test.png" alt="">
                                                    <h4>
                                                        Test
                                                    </h4>
                                                </div>
                                            </li>
                                      </ul>
                                    </div>
                                  </section>
                            </p>
                            <h4>Box precision vs Confusion Matrix</h4>
                            <p>These metrics evaluate distinct aspects of a model's performance.
                                Box precision measures the accuracy of predicted bounding boxes compared to the actual ground truth boxes using
                                IoU (Intersection over Union) as the metric. Confusion matrix precision, on the other hand, focuses on overall
                                classification accuracy across all classes and does not consider the geometric accuracy of predictions. It's
                                important to note that a bounding box can be geometrically accurate (true positive) even if the class prediction is
                                wrong, leading to differences between box precision and confusion matrix precision. </p>
                            <h4>Conclusions</h4>
                            <p>
                                The main goal of the project is to maximize the identification of waste, even at the cost of tolerating some false
                                positives, such as mistakenly identifying debris as waste. Therefore, it is crucial to focus primarily on optimizing the
                                recall. This metric assesses how much of what is actually present is correctly identified, and a high-recall model is
                                prone to label marginally relevant examples.
                                In the context of our project we have a recall of approximately 0.66 means that the model is capturing about 66% of the
                                actual instances of waste.  This is in line with the project's emphasis on maximizing the identification of waste, even if it
                                means accepting some false positives. This trade-off can be acceptable, and the model can be considered successful in achieving its
                                primary objective.
                            </p>
                            </div><!--//content-->
                    </div><!--//section-inner-->                 
                </section><!--//section-->
                
            </div><!--//primary-->
            
			<div class="secondary col-md-4 col-sm-12 col-xs-12">
                <aside class="info aside section">
                    <div class="section-inner">
                        <h2 class="heading">Authors</h2>
                        <div class="content">
                            <p>Bonelli Antonella, 68791</p>
                            <p>Calocero Simona, 68395</p>
                            <p>Rosa Marco, 60315</p>
                        </div><!--//content-->
                    </div><!--//section-inner-->                 
                </aside><!--//aside-->

                <aside class="education aside section">
                    <div class="section-inner">
                        <h2 class="heading">Table of Contents</h2>
                        <div class="content" style="text-align:left;">
                            <p><a href="#introduction">Introduction</a></p>
                            <p><a href="#motivations">Motivations</a></p>
                            <p><a href="#goals">Goals</a></p>
                            <p><a href="#method">Method</a></p>
                            <p><a href="#implementation">Implementation and Code</a></p>
                            <p><a href="#results">Results</a></p>
                        </div><!--//content-->
                    </div><!--//section-inner-->
                </aside><!--//section-->


                <aside class="blog aside section">
                    <div class="section-inner">
                        <h2 class="heading">References and Links</h2>
						<div class="content">
							<div class="item">
								<a href="https://pjreddie.com/darknet/" target="_blank">
                                    Darknet
                                </a>
                            </div>
                            <div class="item">
								<a href="https://pjreddie.com/darknet/yolo/" target="_blank">
                                    YOLO
                                </a>
                            </div>
                            <div class="item">
								<a href="https://docs.roboflow.com/" target="_blank">
                                    Roboflow
                                </a>
                            </div>
                            <div class="item">
                                <a href="https://huggingface.co/docs/datasets/index" target="_blank">
                                    Datasets
                                </a>
                            </div>
                            <div class="item">
								<a href="https://colab.research.google.com/" target="_blank">
                                    Colab
                                </a>
                            </div>
                            <div class="item">
								<a href="http://web.unibas.it/bloisi/corsi/visione-e-percezione.html" target="_blank">
                                    Visione e Percezione course page
                                </a>
							</div>
							<div class="item">
								<a href="http://web.unibas.it/bloisi/" target="_blank">
                                    Domenico Bloisi's home page
                                </a>
							</div>

                        </div><!--//content-->
                    </div><!--//section-inner-->
                </aside><!--//section-->                            
              
            </div><!--//secondary-->    
        </div><!--//row-->
    </div><!--//masonry-->
    
    <!-- ******FOOTER****** --> 
    <footer class="footer">
        <div class="container text-center">
                <small class="copyright">This template adapted from <a href="http://themes.3rdwavemedia.com/" target="_blank">3rd Wave Media</a></small>
        </div><!--//container-->
    </footer><!--//footer-->
</body></html>